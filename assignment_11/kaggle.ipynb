{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle-based Text Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "from nltk.collocations import *\n",
    "\n",
    "import pandas as pd\n",
    "# import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from ast import literal_eval \n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('All I can say is I love this place!  Everyone here is absolutely amazing.  The entire staff is friendly, caring and treat you as if you and your furry friend are like extended family.  \\nI recently moved about 35 miles away from here and I would NEVER change vets.  I still drive here every week.  \\nIf you ever need an honest and caring vet this is the place to come.',\n",
       " 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('yelp_data_official_training.csv', sep='|', low_memory=False)\n",
    "reviews_tmp = list(raw_data['Review Text'])\n",
    "categories = list(raw_data['Category'])\n",
    "data_list = []\n",
    "for i in range(len(categories)):\n",
    "    data_list.append((reviews_tmp[i], categories[i]))\n",
    "random.shuffle(data_list)\n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    '''Split text into sentences and tokenize\n",
    "    '''\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus.replace(\"\\ufeff\", \"\")) \n",
    "    return [nltk.word_tokenize(str(word)) for word in raw_sents]\n",
    "\n",
    "\n",
    "def build_stop_words():\n",
    "    '''Build stop words from SMART (Salton,1971).  Available at ftp://ftp.cs.cornell.edu/pub/smart/english.stop\n",
    "    '''\n",
    "    stop_words = []\n",
    "    file = \"SmartStoplist.txt\"\n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            tmp = fp.readlines()\n",
    "            stop_words = [ word.replace('\\n', '') for word in tmp ]\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "def extract_unigram(sents):\n",
    "    '''Extract unigram candidates, and prune the candidates.\n",
    "\n",
    "    '''\n",
    "    unigram_raw_candidates = []\n",
    "    unigram_candidates = []\n",
    "    lm = WordNetLemmatizer()\n",
    "    for sent in sents:\n",
    "        unigram_raw_candidates += list(ngrams(sent,1))    \n",
    "    \n",
    "    # Unigram pruning: remove punctions, stop words, words that are capitalized in the first character, words less than 2 characters.\n",
    "    stopwords = build_stop_words()\n",
    "    \n",
    "    unigram_pattern = r\"\\d+|\\'+|\\`+|^[A-Z]\\w*|\\w+\\'\\w+|\\.+\"\n",
    "#     unigram_pattern = r\"\\d+|\\'+|\\`+|\\.+\"\n",
    "    for element in unigram_raw_candidates:\n",
    "#         if re.match(unigram_pattern, element[0]) or element[0] in string.punctuation or len(element[0])<2 :        \n",
    "        if re.match(unigram_pattern, element[0]) or element[0] in string.punctuation or element[0] in stopwords or len(element[0])<2 :\n",
    "            continue\n",
    "        else:\n",
    "            unigram_candidates.append(lm.lemmatize(element[0], 'v'))\n",
    "    return unigram_candidates\n",
    "\n",
    "def extract_bigram(sents):\n",
    "    bigram_candidates = []\n",
    "    bigram_raw_candidates = []\n",
    "    for sent in sents:\n",
    "        bigram_raw_candidates += list(ngrams(sent, 2))\n",
    "    \n",
    "    bigram_pattern = r\".*[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]+.*\"                             \n",
    "    for element in bigram_raw_candidates:\n",
    "        element_combined = \" \".join(element)\n",
    "        if re.match(bigram_pattern, element_combined):\n",
    "            continue\n",
    "        else:\n",
    "            bigram_candidates.append(element_combined)\n",
    "    return bigram_candidates\n",
    "\n",
    "\n",
    "def build_backoff_tagger(train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    ngram_tagger = build_backoff_tagger(already_tagged_sents)\n",
    "#     print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n",
    "\n",
    "def clean_output(candidates):\n",
    "    '''Output clean phrases from freqdist format, and limit characters under count.\n",
    "    '''\n",
    "    keyphrases = []\n",
    "    sum = 0\n",
    "    for (keyphrase, count) in candidates:\n",
    "        tmp = list(keyphrase.split())\n",
    "        kp = \" \".join([ word.replace(\"(\", \"\").replace(\"'\", \"\").replace(\",\", \"\") for word in tmp[0::2] ])\n",
    "        keyphrases.append((kp))\n",
    "    return keyphrases\n",
    "\n",
    "# train tagger\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "\n",
    "tagger = train_tagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I am': 1,\n",
       " 'This is': 2,\n",
       " 'a descriptive': 1,\n",
       " 'am going': 1,\n",
       " 'descriptive sentence': 1,\n",
       " 'going to': 1,\n",
       " 'is a': 1,\n",
       " 'is what': 1,\n",
       " 'to test': 1,\n",
       " 'what I': 1}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigram feature\n",
    "def review_features1(review):\n",
    "    feature_dict = {}\n",
    "    try:\n",
    "        # unigram features\n",
    "        words = extract_unigram(tokenize_text(review))\n",
    "        for (word, count) in nltk.FreqDist(words).most_common():\n",
    "            feature_dict[word] = count\n",
    "    except:\n",
    "        print(\"Error occurred for review \")\n",
    "    return feature_dict\n",
    "\n",
    "# unigram feature by Nihar\n",
    "def review_features2(review):\n",
    "    try:\n",
    "        feature_dict = {}\n",
    "        pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "             | \\w+(?:['-]\\w+)*        # words with optional internal hyphens\n",
    "             | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "             | \\.\\.\\.              # ellipsis\n",
    "             | [][.,;\"'?():_`-]    # these are separate tokens; includes ], ['''\n",
    "        words = nltk.regexp_tokenize(review, pattern)\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        norm_words = [word.lower() for word in words if len(word) > 2 and word.lower() not in stopwords]\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "        lemma_words = [wnl.lemmatize(t) for t in norm_words]\n",
    "        freqdist = nltk.FreqDist(lemma_words)\n",
    "        for (word, count) in freqdist.most_common():\n",
    "             feature_dict[word] = count\n",
    "    except: \n",
    "        # print(\"Error occurred for review \" + str(review))\n",
    "        print(\"Error occurred for review \")\n",
    "    return feature_dict\n",
    "\n",
    "# keyphrase chunking feature\n",
    "def review_features3(review):\n",
    "    feature_dict = {}      \n",
    "    # Stem words. Without stemming, top keyphrases consist of phrases with same structure but different forms.\n",
    "    lm = WordNetLemmatizer()\n",
    "    try:\n",
    "        sents = tokenize_text(review)\n",
    "        txt_tagged = []\n",
    "        sents_stemmed = []\n",
    "        for sent in sents:\n",
    "            sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "        for sent in sents_stemmed:\n",
    "            txt_tagged.append(tagger.tag(sent))\n",
    "\n",
    "        # descriptive keyphrases pattern\n",
    "#         {(<JJ>* <NN.*>+ <IN>)? <JJ>+ <NN.*>+}\n",
    "        dkp = nltk.RegexpParser('DK: {(<JJ>|<NN>)+(<NN>|<IN>)|<NN>}') \n",
    "\n",
    "        descriptive_keyphrases = []\n",
    "        for sent in txt_tagged:\n",
    "            tree = dkp.parse(sent)\n",
    "            for subtree in tree.subtrees():\n",
    "                if subtree.label() == 'DK': \n",
    "                    dk = \" \".join(str(e) for e in subtree[0:])\n",
    "                    descriptive_keyphrases.append(dk)\n",
    "\n",
    "        for (keyphrase, count) in nltk.FreqDist(descriptive_keyphrases).most_common():\n",
    "            tmp = list(keyphrase.split())\n",
    "            kp = \" \".join([ str(word).replace(\"(\", \"\").replace(\"'\", \"\").replace(\",\", \"\") for word in tmp[0::2] ])\n",
    "            feature_dict[kp] = count\n",
    "    except:\n",
    "        print(\"Error occurred for review \")\n",
    "    return feature_dict\n",
    "\n",
    "# collocation feature\n",
    "def review_features4(review):\n",
    "    feature_dict = {}\n",
    "    try:\n",
    "        # corpus_words = nltk.word_tokenize(corpus)\n",
    "        sents = tokenize_text(review)\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "        # stem sentences\n",
    "        lm = WordNetLemmatizer()\n",
    "        sents_stemmed = []\n",
    "        for sent in sents:\n",
    "            sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "        candidates_tmp = []\n",
    "        for sent in sents_stemmed:\n",
    "            # tag collocation words with trained brown tagger\n",
    "            finder = BigramCollocationFinder.from_words(tagger.tag(sent))\n",
    "            tmp = finder.nbest(bigram_measures.pmi, 10000)\n",
    "            candidates_tmp += tmp\n",
    "\n",
    "        candidates_2 = nltk.FreqDist(candidates_tmp).most_common()\n",
    "        stopwords = build_stop_words()\n",
    "        candidates_ = []\n",
    "        for candidate in candidates_2:\n",
    "            # filter out descriptive noun phrases\n",
    "            if re.match(r'NN.*', candidate[0][1][1]):\n",
    "                # filter out punctuation, stopwords\n",
    "                if  (candidate[0][0][0] not in string.punctuation) and (candidate[0][0][0] not in stopwords) and (candidate[0][1][0] not in stopwords):\n",
    "                    kp = candidate[0][0][0] + \" \" + candidate[0][1][0]\n",
    "                    feature_dict[kp] = candidate[1]\n",
    "    except:\n",
    "        print(\"Error occurred for review\")\n",
    "    return feature_dict\n",
    "\n",
    "# biagram feature\n",
    "def review_features5(review):\n",
    "    feature_dict = {}\n",
    "    try:\n",
    "        words = extract_bigram(tokenize_text(review))\n",
    "        for (word, count) in nltk.FreqDist(words).most_common():\n",
    "            feature_dict[word] = count\n",
    "    except:\n",
    "        print(\"Biagram feature error.\")\n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "\n",
    "def review_features(review):\n",
    "    unigram = review_features1(review)\n",
    "    chunking = review_features5(review)\n",
    "#     collocation = review_features4(review)\n",
    "    features = unigram.copy()\n",
    "    features.update(chunking)\n",
    "#     features.update(collocation)\n",
    "    return features\n",
    "\n",
    "# review_features5(\"This is what I am going to test. This is a descriptive sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for review \n",
      "Error occurred for review \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'absolutely': 1,\n",
       "   'amaze': 1,\n",
       "   'care': 2,\n",
       "   'change': 1,\n",
       "   'drive': 1,\n",
       "   'entire': 1,\n",
       "   'extend': 1,\n",
       "   'family': 1,\n",
       "   'friend': 1,\n",
       "   'friendly': 1,\n",
       "   'furry': 1,\n",
       "   'honest': 1,\n",
       "   'love': 1,\n",
       "   'miles': 1,\n",
       "   'move': 1,\n",
       "   'place': 2,\n",
       "   'recently': 1,\n",
       "   'staff': 1,\n",
       "   'treat': 1,\n",
       "   'vet': 2,\n",
       "   'week': 1},\n",
       "  5),\n",
       " ({'absolutely': 1,\n",
       "   'amaze': 1,\n",
       "   'amount': 1,\n",
       "   'ca': 1,\n",
       "   'careful': 1,\n",
       "   'choose': 1,\n",
       "   'clean': 1,\n",
       "   'comfortable': 1,\n",
       "   'courteous': 1,\n",
       "   'dermatologists': 1,\n",
       "   'drop': 1,\n",
       "   'experience': 1,\n",
       "   'expertise': 1,\n",
       "   'facility': 1,\n",
       "   'feel': 2,\n",
       "   'felt': 1,\n",
       "   'good': 1,\n",
       "   'great': 1,\n",
       "   'group': 1,\n",
       "   'hand': 1,\n",
       "   'health': 1,\n",
       "   'high': 1,\n",
       "   'knowledgeable': 1,\n",
       "   'level': 1,\n",
       "   'mother': 1,\n",
       "   'pick': 1,\n",
       "   'prior': 1,\n",
       "   'professional': 2,\n",
       "   'quality': 1,\n",
       "   'receive': 1,\n",
       "   'recommendation': 1,\n",
       "   'referrals': 1,\n",
       "   'research': 1,\n",
       "   'service': 2,\n",
       "   'situations': 1,\n",
       "   'slack': 1,\n",
       "   'staff': 1,\n",
       "   'start': 2,\n",
       "   'trust': 1,\n",
       "   'uncomfortable': 1},\n",
       "  3),\n",
       " ({'bore': 1,\n",
       "   'bring': 1,\n",
       "   'carousel': 1,\n",
       "   'chime': 1,\n",
       "   'clock': 4,\n",
       "   'dot': 1,\n",
       "   'fair': 1,\n",
       "   'fantastic': 1,\n",
       "   'figure': 1,\n",
       "   'gander': 1,\n",
       "   'good': 1,\n",
       "   'grandfather': 1,\n",
       "   'guy': 1,\n",
       "   'handle': 1,\n",
       "   'hear': 1,\n",
       "   'hour': 1,\n",
       "   'miniatures': 1,\n",
       "   'neat': 2,\n",
       "   'oddities': 1,\n",
       "   'oogle': 1,\n",
       "   'pop': 1,\n",
       "   'price': 1,\n",
       "   'repair': 2,\n",
       "   'run': 1,\n",
       "   'store': 1},\n",
       "  4),\n",
       " ({'animals': 1,\n",
       "   'anymore': 1,\n",
       "   'asleep': 1,\n",
       "   'awful': 1,\n",
       "   'back': 1,\n",
       "   'bone': 1,\n",
       "   'busy': 1,\n",
       "   'cab': 2,\n",
       "   'car': 1,\n",
       "   'care': 2,\n",
       "   'chair': 1,\n",
       "   'chin': 1,\n",
       "   'come': 1,\n",
       "   'communication': 1,\n",
       "   'concern': 1,\n",
       "   'conduct': 1,\n",
       "   'cut': 1,\n",
       "   'define': 1,\n",
       "   'dog': 2,\n",
       "   'drink': 1,\n",
       "   'employee': 1,\n",
       "   'employees': 1,\n",
       "   'empty': 1,\n",
       "   'establishment': 1,\n",
       "   'experience': 1,\n",
       "   'fall': 2,\n",
       "   'friend': 7,\n",
       "   'frustrate': 1,\n",
       "   'fuck': 2,\n",
       "   'head': 2,\n",
       "   'home': 1,\n",
       "   'hospital': 4,\n",
       "   'hour': 1,\n",
       "   'hours': 2,\n",
       "   'idea': 1,\n",
       "   'indecent': 1,\n",
       "   'injure': 1,\n",
       "   'injuries': 1,\n",
       "   'injury': 2,\n",
       "   'institution': 1,\n",
       "   'knee': 1,\n",
       "   'lack': 2,\n",
       "   'life': 1,\n",
       "   'lobby': 1,\n",
       "   'manner': 1,\n",
       "   'meantime': 1,\n",
       "   'medical': 1,\n",
       "   'mention': 1,\n",
       "   'minor': 1,\n",
       "   'minutes': 1,\n",
       "   'neck': 2,\n",
       "   'need': 1,\n",
       "   'nose': 1,\n",
       "   'owners': 1,\n",
       "   'persons': 1,\n",
       "   'practically': 1,\n",
       "   'purgatory': 1,\n",
       "   'push': 1,\n",
       "   'release': 1,\n",
       "   'remember': 1,\n",
       "   'room': 1,\n",
       "   'rule': 1,\n",
       "   'sensitive': 1,\n",
       "   'shrug': 1,\n",
       "   'sick': 1,\n",
       "   'similar': 2,\n",
       "   'sit': 1,\n",
       "   'spine': 2,\n",
       "   'stay': 1,\n",
       "   'stitch': 1,\n",
       "   'suck': 1,\n",
       "   'suffer': 2,\n",
       "   'surgical': 1,\n",
       "   'tad': 1,\n",
       "   'tell': 2,\n",
       "   'term': 1,\n",
       "   'terrible': 2,\n",
       "   'test': 1,\n",
       "   'things': 1,\n",
       "   'threaten': 1,\n",
       "   'time': 1,\n",
       "   'treatment': 2,\n",
       "   'uncomfortable': 1,\n",
       "   'unnecessary': 1,\n",
       "   'wait': 5,\n",
       "   'wheelchair': 1,\n",
       "   'woman': 1},\n",
       "  3),\n",
       " ({'empathy': 1,\n",
       "   'finally': 1,\n",
       "   'forbid': 1,\n",
       "   'front': 1,\n",
       "   'give': 1,\n",
       "   'hard': 1,\n",
       "   'hour': 1,\n",
       "   'hours': 1,\n",
       "   'information': 1,\n",
       "   'lack': 1,\n",
       "   'nurse': 1,\n",
       "   'result': 3,\n",
       "   'status': 1,\n",
       "   'test': 2,\n",
       "   'time': 1,\n",
       "   'total': 1},\n",
       "  3),\n",
       " ({'customer': 1,\n",
       "   'eager': 1,\n",
       "   'info': 1,\n",
       "   'key': 1,\n",
       "   'need': 1,\n",
       "   'offer': 1,\n",
       "   'piece': 1,\n",
       "   'possess': 1,\n",
       "   'purchase': 1,\n",
       "   'service': 1},\n",
       "  1),\n",
       " ({'dog': 1,\n",
       "   'girls': 1,\n",
       "   'great': 1,\n",
       "   'groomers': 1,\n",
       "   'hair': 1,\n",
       "   'highly': 1,\n",
       "   'huge': 1,\n",
       "   'job': 1,\n",
       "   'lot': 1,\n",
       "   'miracle': 1,\n",
       "   'nail': 1,\n",
       "   'place': 1,\n",
       "   'price': 1,\n",
       "   'reasonable': 1,\n",
       "   'recommend': 1,\n",
       "   'stellar': 1,\n",
       "   'tolerate': 1,\n",
       "   'trim': 1,\n",
       "   'wonderful': 1},\n",
       "  6),\n",
       " ({'aesthetician': 1,\n",
       "   'body': 1,\n",
       "   'business': 1,\n",
       "   'cosplay': 1,\n",
       "   'fail': 1,\n",
       "   'frequently': 1,\n",
       "   'full': 1,\n",
       "   'hobby': 1,\n",
       "   'job': 1,\n",
       "   'part': 1,\n",
       "   'professional': 1,\n",
       "   'repeat': 1,\n",
       "   'time': 1,\n",
       "   'trust': 1,\n",
       "   'wax': 1,\n",
       "   'win': 1},\n",
       "  1),\n",
       " ({'announce': 1,\n",
       "   'apartment': 3,\n",
       "   'ban': 1,\n",
       "   'bed': 1,\n",
       "   'behavior': 1,\n",
       "   'bet': 1,\n",
       "   'body': 1,\n",
       "   'bother': 1,\n",
       "   'business': 1,\n",
       "   'campus': 1,\n",
       "   'coffee': 1,\n",
       "   'communications': 1,\n",
       "   'community': 1,\n",
       "   'contact': 1,\n",
       "   'deep': 1,\n",
       "   'demand': 1,\n",
       "   'dont': 1,\n",
       "   'engineer': 1,\n",
       "   'enter': 1,\n",
       "   'extend': 1,\n",
       "   'feel': 1,\n",
       "   'find': 1,\n",
       "   'fyi': 1,\n",
       "   'government': 1,\n",
       "   'grapevine': 1,\n",
       "   'guy': 1,\n",
       "   'happen': 2,\n",
       "   'hear': 1,\n",
       "   'heart': 2,\n",
       "   'hospital': 1,\n",
       "   'issue': 1,\n",
       "   'lay': 1,\n",
       "   'legal': 1,\n",
       "   'libertarian': 1,\n",
       "   'life': 1,\n",
       "   'line': 1,\n",
       "   'live': 3,\n",
       "   'lot': 1,\n",
       "   'maintenance': 1,\n",
       "   'mean': 1,\n",
       "   'million': 1,\n",
       "   'minute': 1,\n",
       "   'morning': 1,\n",
       "   'move': 1,\n",
       "   'overboard': 1,\n",
       "   'past': 1,\n",
       "   'people': 3,\n",
       "   'place': 4,\n",
       "   'point': 1,\n",
       "   'policy': 1,\n",
       "   'radar': 1,\n",
       "   'random': 1,\n",
       "   'renters': 1,\n",
       "   'research': 1,\n",
       "   'restrict': 1,\n",
       "   'rule': 1,\n",
       "   'secretly': 1,\n",
       "   'sell': 1,\n",
       "   'set': 1,\n",
       "   'smoke': 6,\n",
       "   'smoker': 1,\n",
       "   'smokers': 1,\n",
       "   'stand': 1,\n",
       "   'start': 1,\n",
       "   'style': 1,\n",
       "   'suspect': 1,\n",
       "   'suspicion': 1,\n",
       "   'warn': 1,\n",
       "   'whatsoever': 1},\n",
       "  2),\n",
       " ({'appointment': 1,\n",
       "   'back': 1,\n",
       "   'book': 1,\n",
       "   'buck': 1,\n",
       "   'day': 1,\n",
       "   'docs': 1,\n",
       "   'expert': 1,\n",
       "   'fantastic': 1,\n",
       "   'feel': 1,\n",
       "   'full': 1,\n",
       "   'lead': 1,\n",
       "   'make': 2,\n",
       "   'million': 1,\n",
       "   'pretty': 1,\n",
       "   'quick': 1,\n",
       "   'room': 1,\n",
       "   'schedule': 1,\n",
       "   'wiggle': 1},\n",
       "  3)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(review_features1(key), value) for (key, value) in data_list ]\n",
    "# featuresets5 = [(review_features5(key), value) for (key, value) in data_list ]\n",
    "# featuresets = featuresets1 + featuresets5\n",
    "featuresets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_sets (feature_function, items, return_items=False):\n",
    "    # Create the features sets.  Call the function that was passed in.\n",
    "    # For names data, key is the name, and value is the gender\n",
    "    featuresets = [(feature_function(key), value) for (key, value) in items]\n",
    "    \n",
    "    # Divided training and testing in thirds.  Could divide in other proportions instead.\n",
    "    split = int(float(len(featuresets)) * 3/ 4.0)\n",
    "    \n",
    "    train_set, dev_set = featuresets[0:split], featuresets[split:]\n",
    "    train_items, dev_items = items[0:split], items[split:]\n",
    "    if return_items == True:\n",
    "        return train_set, dev_set, train_items, dev_items\n",
    "    else:\n",
    "        return train_set, dev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for review \n",
      "Biagram feature error.\n",
      "Error occurred for review \n",
      "Biagram feature error.\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set = create_training_sets (review_features, data_list, return_items=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669\n"
     ]
    }
   ],
   "source": [
    "cl = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (\"%.3f\" % nltk.classify.accuracy(cl, dev_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('yelp_data_official_test_nocategories.csv', sep='|', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_test = list(test_data['Review Text'])\n",
    "id_test = list(test_data['ID'])\n",
    "test_list = []\n",
    "for i in range(len(id_test)):\n",
    "    test_list.append((id_test[i], reviews_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"yelp_data_official_test_submission_leon.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Category\\n\")\n",
    "    for (id_,review) in test_list:\n",
    "        category = cl.classify(review_features(review))\n",
    "        f.write(str(id_) + \",\" + str(category) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
