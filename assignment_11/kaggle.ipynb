{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle-based Text Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "# import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from ast import literal_eval \n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "# analysis\n",
    "# import statsmodels.api as sm\n",
    "# from scipy.stats import pearsonr\n",
    "# from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "# from sklearn.metrics import roc_curve, auc, classification_report\n",
    "# from sklearn.preprocessing import normalize, scale, StandardScaler, Normalizer\n",
    "# from sklearn.pipeline import Pipeline, make_pipeline\n",
    "# from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('yelp_data_official_training.csv', sep='|', low_memory=False)\n",
    "beauty_spa = \"\"\n",
    "home_services = \"\"\n",
    "health_medical = \"\"\n",
    "local_services = \"\"\n",
    "veterinarians = \"\"\n",
    "pets = \"\"\n",
    "\n",
    "reviews_tmp = list(raw_data['Review Text'])\n",
    "categories = list(raw_data['Category'])\n",
    "for i in range(len(categories)):\n",
    "    if categories[i] == 1:\n",
    "        beauty_spa += \" \" + str(reviews_tmp[i])\n",
    "    elif categories[i] == 2:\n",
    "        home_services += \" \" + str(reviews_tmp[i])\n",
    "    elif categories[i] == 3:\n",
    "        health_medical += \" \" + str(reviews_tmp[i])\n",
    "    elif categories[i] == 4:\n",
    "        local_services += \" \" + str(reviews_tmp[i])\n",
    "    elif categories[i] == 5:\n",
    "        veterinarians += \" \" + str(reviews_tmp[i])\n",
    "    elif categories[i] == 6:\n",
    "        pets += str(reviews_tmp[i])\n",
    "\n",
    "reviews = [beauty_spa, home_services, health_medical, local_services, veterinarians, pets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    '''Split text into sentences and tokenize\n",
    "    '''\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus.replace(\"\\ufeff\", \"\")) \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "\n",
    "def build_stop_words():\n",
    "    '''Build stop words from SMART (Salton,1971).  Available at ftp://ftp.cs.cornell.edu/pub/smart/english.stop\n",
    "    '''\n",
    "    stop_words = []\n",
    "    file = \"SmartStoplist.txt\"\n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            tmp = fp.readlines()\n",
    "            stop_words = [ word.replace('\\n', '') for word in tmp ]\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "def extract_unigram(sents):\n",
    "    '''Extract unigram candidates, and prune the candidates.\n",
    "\n",
    "    '''\n",
    "    unigram_raw_candidates = []\n",
    "    unigram_candidates = []\n",
    "    lm = WordNetLemmatizer()\n",
    "    for sent in sents:\n",
    "        unigram_raw_candidates += list(ngrams(sent,1))    \n",
    "    \n",
    "    # Unigram pruning: remove punctions, stop words, words that are capitalized in the first character, words less than 2 characters.\n",
    "    stopwords = build_stop_words()\n",
    "  \n",
    "    unigram_pattern = r\"\\d+|\\'+|\\`+|^[A-Z]\\w*|\\w+\\'\\w+|\\.+\"\n",
    "    for element in unigram_raw_candidates:\n",
    "        if re.match(unigram_pattern, element[0]) or element[0] in string.punctuation or element[0] in stopwords or len(element[0])<2 :\n",
    "            continue\n",
    "        else:\n",
    "            unigram_candidates.append(lm.lemmatize(element[0], 'v'))\n",
    "    return unigram_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81622\n"
     ]
    }
   ],
   "source": [
    "category_data = []\n",
    "\n",
    "i = 0\n",
    "for element in range(6):\n",
    "    for word in set(extract_unigram(tokenize_text(reviews[i]))):\n",
    "        category_data.append(({'unigram': word}, i+1))\n",
    "    i += 1\n",
    "random.shuffle(category_data)\n",
    "\n",
    "print(len(category_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.115\n"
     ]
    }
   ],
   "source": [
    "cl = nltk.NaiveBayesClassifier.train(category_data[:40000])\n",
    "print (\"%.3f\" % nltk.classify.accuracy(cl, category_data[40001:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('yelp_data_official_test_nocategories.csv', sep='|', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_file(content):\n",
    "    with open('first-submit.csv', 'a') as fp:\n",
    "        for (n, v) in content:\n",
    "            line = str(n) + ',' + str(v)\n",
    "            fp.write(line)\n",
    "            fp.write('\\n')\n",
    "        fp.close()\n",
    "        \n",
    "id = 0\n",
    "content = []\n",
    "for test_reviews in list(test_data['Review Text']):\n",
    "    extract_unigrams = set(extract_unigram(tokenize_text(test_reviews)))\n",
    "    results = []\n",
    "    for element in extract_unigrams:\n",
    "        features = {\n",
    "            'unigram': element\n",
    "        }\n",
    "        results.append(cl.classify(features))\n",
    "    for (category, counts) in nltk.FreqDist(results).most_common(1):\n",
    "        content.append((id, category))\n",
    "    id += 1\n",
    "    \n",
    "write_file(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
