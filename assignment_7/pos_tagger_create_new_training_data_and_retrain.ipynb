{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to retrain the NLTK backoff tagger.\n",
    "- You'll see an example in which some recipe text has some errors in tagging, most likely because the training data did not have many examples of the target sentence structure.  \n",
    "- Next, you'll see the affects of adding a few sentences of training data with the missing sentence structure on the accuracy of the tagger.\n",
    "- Your assignment is to do something similar on your adopted text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for training and evaluating a backoff tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger(train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a specialized function for training a tagger on the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown():\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "    return train_tagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for creating an NLTK corpus object, so we can operate on it using nltk.tokenize_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences    \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_corpus(f):\n",
    "    with open(f, 'r') as text_file:\n",
    "        new_corpus = text_file.read()\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and evaluate an ngram backoff tagger, using the brown corpus as the training and testing set.  (This takes a few moments to complete.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagger = train_tagger_on_brown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, read in a file of recipes and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cookbook_file = './the_history_of_north_america.txt'\n",
    "cookbook_sents = tokenize_text(create_corpus(cookbook_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this collection,  imperative sentences (sentences that being with a verb) are always mistagged.  The POS tagger marks the initial verb as NN instead of VB.  (There may be other kinds of errors too, but we are only looking at imperative sentences here.) In order to see the sentences where the errors are occuring, the code below finds sentences that begin with imperatives, tags them with the tagger, and returns them in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cookbook_imperatives(sents, tagger):\n",
    "    cooking_commands = [\"Wash\", \"Stir\", \"Moisten\", \"Drain\", \"Cook\", \"Pour\", \"Chop\", \"Slice\", \"Season\", \"Mix\", \"Fry\", \"Bake\", \"Roast\", \"Wisk\"]        \n",
    "    return [tagger.tag(sent) for sent in sents if sent[0] in cooking_commands]       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imperatives = get_cookbook_imperatives(cookbook_sents, brown_tagger)\n",
    "imperatives[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that most of the initial words are incorrectly tagged as nouns rather than verbs.  How can we fix this?  One way is to label a few rather generic sentences with the structure we are interested in, add them to the start of the training data, and then retrain the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_tagger_on_brown_augmented_with_cooking_sents():\n",
    "\n",
    "    cooking_action_sents = [[('Strain', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Mix', 'VB'), ('them', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Season', 'VB'), ('them', 'PPS'), ('with', 'IN'), ('pepper', 'NN'), ('.', '.')], \n",
    "                        [('Wash', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Chop', 'VB'), ('the', 'AT'), ('greens', 'NNS'), ('.', '.')],\n",
    "                        [('Slice', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')],\n",
    "                        [('Bake', 'VB'), ('the', 'AT'), ('cake', 'NN'), ('.', '.')],\n",
    "                        [('Pour', 'VB'), ('into', 'IN'), ('a', 'AT'), ('mold', 'NN'), ('.', '.')],\n",
    "                        [('Stir', 'VB'), ('the', 'AT'), ('mixture', 'NN'), ('.', '.')],\n",
    "                        [('Moisten', 'VB'), ('the', 'AT'), ('grains', 'NNS'), ('.', '.')],\n",
    "                        [('Cook', 'VB'), ('the', 'AT'), ('duck', 'NN'), ('.', '.')],\n",
    "                        [('Drain', 'VB'), ('for', 'IN'), ('one', 'CD'), ('day', 'NN'), ('.', '.')]]\n",
    "\n",
    "#     my_action_sents = [[('Strain', 'VB'), ('it', 'PPS'), ('well', 'RB'), ('.', '.')]]\n",
    "    brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "    \n",
    "    #append hand-tagged cooking sentences to the front of the training data\n",
    "    all_tagged_sents = cooking_action_sents + brown_tagged_sents\n",
    "    return train_tagger(all_tagged_sents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_and_cooking_tagger = train_tagger_on_brown_augmented_with_cooking_sents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well is this working on the cookbook imperatives now? Is more training data needed to change the behavior of the tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "better_imperatives = get_cookbook_imperatives(cookbook_sents, brown_and_cooking_tagger)\n",
    "better_imperatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked quite well.  It would be worth experimenting to see if it would still work if I'd supplied fewer of the cooking verbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assignment:##\n",
    "\n",
    "Rewrite this notebook to do the following:\n",
    "- Tag your adopted text with an NLTK backoff tagger\n",
    "- Identify a common type of error that is amenable to fix by making a pattern of training data, similar to what we see with the recipe examples.  You'll want to focus on a particular pattern so that making a few tweaks will have a impact on the results of training.\n",
    "- Show the before and after effects on the output of the tagger.  Ideally you'll see the errors get fixed not just on the specific examples you fixed, but on similar examples with different words.  In the case of recipes, imperative verbs beyond those in the hardcoded list would be fixed because the tagger would recognize the pattern that verbs can occur at the start of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Ag-', 'NN'),\n",
       " ('gressive', 'NN'),\n",
       " ('policy', 'NN'),\n",
       " ('toward', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('Indians', 'NPS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mybook = \"the_history_of_north_america.txt\"\n",
    "mybook_sents = tokenize_text(create_corpus(mybook))\n",
    "trained_tagger = train_tagger_on_brown()\n",
    "tagged = trained_tagger.tag(mybook_sents[100])\n",
    "tagged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
