{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrases Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "\n",
    "In this section, the corpus is created either from a text file or a url, which is then split into sentences and tokenized. Other functions include build tagger based on brown corpus, and clean outut under certain characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def create_corpus_from_file(file):\n",
    "    '''Create corpus from file\n",
    "    ''' \n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            corpus = fp.read()\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        corpus = \"\"\n",
    "    return corpus\n",
    "\n",
    "def create_corpus_from_url(url):\n",
    "    '''Create corpus from url\n",
    "    ''' \n",
    "    try:\n",
    "        res = urllib.request.urlopen(url)\n",
    "        corpus = res.read().decode('utf-8')\n",
    "    except:\n",
    "        corpus = \"\"\n",
    "        print(\"Can't open url: {0}\".format(url))\n",
    "    finally:\n",
    "        res.close()\n",
    "    return corpus   \n",
    "\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    '''Split text into sentences and tokenize\n",
    "    '''\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus.replace(\"\\ufeff\", \"\")) \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def build_backoff_tagger(train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    ngram_tagger = build_backoff_tagger(already_tagged_sents)\n",
    "#     print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n",
    "\n",
    "# train tagger\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "\n",
    "tagger = train_tagger(brown_tagged_sents)\n",
    "\n",
    "\n",
    "def clean_output(candidates, limit=0):\n",
    "    '''Output clean phrases from freqdist format, and limit characters under count.\n",
    "    '''\n",
    "    keyphrases = []\n",
    "    sum = 0\n",
    "    for (keyphrase, count) in candidates:\n",
    "        tmp = list(keyphrase.split())\n",
    "        kp = \" \".join([ word.replace(\"(\", \"\").replace(\"'\", \"\").replace(\",\", \"\") for word in tmp[0::2] ])\n",
    "        sum += len(kp)\n",
    "        if sum < limit:\n",
    "            keyphrases.append(kp)\n",
    "        else:\n",
    "            return keyphrases\n",
    "    return keyphrases\n",
    "\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "file = \"pride_and_prejudice.txt\"\n",
    "corpus = create_corpus_from_file(file)\n",
    "sents = tokenize_text(corpus)\n",
    "# corpus = create_corpus_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Showing Frequent Terms \n",
    "\n",
    "In section 1, I tried to extract phrases that appear the most in text file with unigram segamentation. The unigrame candidates are pruned by removing punctions, stop words (with some extra modal words), proper noune, and words less than 2 characters, and also by stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time', 'sister', 'family', 'man', 'father', 'day', 'letter', 'mother', 'reply', 'room', 'friend', 'return', 'house', 'manner', 'work', 'love', 'answer', 'pleasure', 'cry', 'subject', 'part', 'aunt', 'daughter', 'dear', 'morning', 'place', 'walk', 'word', 'sisters', 'talk', 'ladies', 'brother', 'happiness', 'surprise', 'party', 'opinion', 'attention', 'eye', 'character', 'moment', 'world', 'uncle', 'marriage', 'town', 'smile', 'conversation', 'kind', 'mind', 'woman', 'affection', 'side', 'point', 'dance', 'life', 'reason', 'turn', 'object', 'behaviour', 'friends', 'person', 'husband', 'lady', 'honour', 'call', 'acquaintance', 'look', 'delight', 'people', 'occasion', 'daughters', 'hope', 'cousin', 'idea', 'pass', 'doubt', 'spirit', 'power', 'business', 'interest', 'wife', 'pride', 'manners', 'heart', 'visit', 'carriage', 'account', 'address', 'girls', 'concern', 'promise', 'fear', 'endeavour', 'question', 'ladyship', 'term', 'thing', 'advantage', 'country', 'civility', 'respect', 'ball', 'compliment', 'oblige', 'fortune', 'officer', 'gentlemen', 'girl', 'laugh', 'care', 'situation', 'sense', 'leave', 'gentleman', 'form', 'minutes', 'matter', 'sort']\n"
     ]
    }
   ],
   "source": [
    "def build_stop_words():\n",
    "    '''Build stop words from SMART (Salton,1971).  Available at ftp://ftp.cs.cornell.edu/pub/smart/english.stop\n",
    "    '''\n",
    "    stop_words = []\n",
    "    file = \"SmartStoplist.txt\"\n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            tmp = fp.readlines()\n",
    "            stop_words = [ word.replace('\\n', '') for word in tmp ]\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "def extract_unigram(sents):\n",
    "    '''Extract unigram candidates, and prune the candidates.\n",
    "\n",
    "    '''\n",
    "    unigram_raw_candidates = []\n",
    "    unigram_candidates = []\n",
    "    lm = WordNetLemmatizer()\n",
    "    for sent in sents:\n",
    "        unigram_raw_candidates += list(ngrams(sent,1))    \n",
    "    \n",
    "    # Unigram pruning: remove punctions, stop words, words that are capitalized in the first character, words less than 2 characters.\n",
    "    stopwords = build_stop_words()\n",
    "  \n",
    "    unigram_pattern = r\"\\d+|\\'+|\\`+|^[A-Z]\\w*\"\n",
    "    for element in unigram_raw_candidates:\n",
    "        if re.match(unigram_pattern, element[0]) or element[0] in string.punctuation or element[0] in stopwords or len(element[0])<2 :\n",
    "            continue\n",
    "        else:\n",
    "            unigram_candidates.append(lm.lemmatize(element[0], 'v'))\n",
    "    return unigram_candidates\n",
    "\n",
    "\n",
    "raw_candidates = extract_unigram(sents)\n",
    "tagged_candidates = tagger.tag(raw_candidates)\n",
    "candidates = []\n",
    "for (word, pos) in tagged_candidates:\n",
    "    if re.match(r'NN.*', pos):\n",
    "        candidates.append(word)\n",
    "\n",
    "candidates_1 = nltk.FreqDist(candidates).most_common(700)\n",
    "print(clean_output(candidates_1, 700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collocations with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr. darcy', 'mrs. bennet', 'mr. collins', 'lady catherine', 'mr. bingley', 'mr. bennet', 'miss bingley', 'mr. wickham', 'miss bennet', 'elizabeth wa', 'mrs. gardiner', 'sir william', 'young lady', 'project gutenberg-tm', 'de bourgh', 'miss darcy', 'young man', 'bennet wa', 'mr. gardiner', 'colonel fitzwilliam', 'mrs. collins', 'bingley wa', 'colonel forster', 'cried elizabeth', 'electronic work', 'miss lucas', 'great deal', 'mrs. hurst']\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "# corpus_words = nltk.word_tokenize(corpus)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# stem sentences\n",
    "lm = WordNetLemmatizer()\n",
    "sents_stemmed = []\n",
    "for sent in sents:\n",
    "    sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "candidates_tmp = []\n",
    "for sent in sents_stemmed:\n",
    "    # tag collocation words with trained brown tagger\n",
    "    finder = BigramCollocationFinder.from_words(tagger.tag(sent))\n",
    "    tmp = finder.nbest(bigram_measures.pmi, 700)\n",
    "    candidates_tmp += tmp\n",
    "\n",
    "candidates_2 = nltk.FreqDist(candidates_tmp).most_common(700)\n",
    "\n",
    "stopwords = build_stop_words()\n",
    "candidates_ = []\n",
    "limit = 700\n",
    "sum = 0\n",
    "for candidate in candidates_2:\n",
    "    # filter out descriptive noun phrases\n",
    "    if re.match(r'NN.*', candidate[0][1][1]):\n",
    "        # filter out punctuation, stopwords\n",
    "        if  (candidate[0][0][0] not in string.punctuation) and (candidate[0][0][0] not in stopwords) and (candidate[0][1][0] not in stopwords):\n",
    "            kp = candidate[0][0][0] + \" \" + candidate[0][1][0]\n",
    "            sum += len(kp)\n",
    "            if sum < limit:\n",
    "                candidates_.append(kp)\n",
    "            else:\n",
    "                break\n",
    "print(candidates_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information obtained from Syntax aka Partial Parsing (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['young lady', 'young man', 'electronic work', 'great deal', 'dear lizzy', 'whole party', 'sure i', 'literary archive foundation', 'dear sir', 'good humour', 'great pleasure', 'good opinion', 'dear aunt', 'young men', 'good deal', 'young woman', 'dear jane', 'fair cousin', 'own family', 'own feeling', 'dear mr. bennet', 'whole family', 'late mr. darcy', 'own room', 'dear wickham', 'dear lydia', 'dear sister', 'full project gutenberg-tm license', 'real character', 'such thing', 'own child', 'short pause', 'large party', 'dear madam', 'public domain', 'intimate friend', 'perfect indifference', 'good heaven', 'dear father', 'own way', 'humble abode', 'present i', 'good news', 'good spirit', 'such term', 'dear charlotte', 'slight bow', 'such circumstance', 'good sense', 'sweet girl', 'short time', 'dear miss elizabeth', 'different manner', 'dear eliza', 'young person', 'poor mother', 'human nature', 'short silence']\n"
     ]
    }
   ],
   "source": [
    "txt_tagged = []\n",
    "\n",
    "# Stem words. Without stemming, top keyphrases consist of phrases with same structure but different forms.\n",
    "lm = WordNetLemmatizer()\n",
    "sents_stemmed = []\n",
    "for sent in sents:\n",
    "    sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "for sent in sents_stemmed:\n",
    "    txt_tagged.append(tagger.tag(sent))\n",
    "\n",
    "# descriptive keyphrases pattern\n",
    "dkp = nltk.RegexpParser('DK: {(<JJ>* <NN.*>+ <IN>)? <JJ>+ <NN.*>+}') \n",
    "\n",
    "descriptive_keyphrases = []\n",
    "for sent in txt_tagged:\n",
    "    tree = dkp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'DK': \n",
    "            dk = \" \".join(str(e) for e in subtree[0:])\n",
    "            descriptive_keyphrases.append(dk)\n",
    "\n",
    "\n",
    "            \n",
    "candidates = nltk.FreqDist(descriptive_keyphrases).most_common(200)\n",
    "print(clean_output(candidates, 700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinple unigram approach does not work well, since the goal of extracting descriptive keyphrases maynot be reflected by a single noun word. \n",
    "\n",
    "The top output of Collocations approach contains title nouns, which can't represent key phrases.\n",
    "\n",
    "Chunking phrase seems to work better, but some keyphrases may have same duplicated marning, such as \"young lady\" and \"young woman\" both are keyphrases extracted, but to some extent, have same meaning. \n",
    "\n",
    "Based on three approaches I tried, I think chunking works best among the three, but still need extra work on thesaurus analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
