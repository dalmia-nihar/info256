{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrases Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "\n",
    "In this section, the corpus is created either from a text file or a url, which is then split into sentences and tokenized. Other functions include build tagger based on brown corpus, and clean outut under certain characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def create_corpus_from_file(file):\n",
    "    '''Create corpus from file\n",
    "    ''' \n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            corpus = fp.read()\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        corpus = \"\"\n",
    "    return corpus\n",
    "\n",
    "def create_corpus_from_url(url):\n",
    "    '''Create corpus from url\n",
    "    ''' \n",
    "    try:\n",
    "        res = urllib.request.urlopen(url)\n",
    "        corpus = res.read().decode('utf-8')\n",
    "    except:\n",
    "        corpus = \"\"\n",
    "        print(\"Can't open url: {0}\".format(url))\n",
    "    finally:\n",
    "        res.close()\n",
    "    return corpus   \n",
    "\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    '''Split text into sentences and tokenize\n",
    "    '''\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus.replace(\"\\ufeff\", \"\")) \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def build_backoff_tagger(train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    ngram_tagger = build_backoff_tagger(already_tagged_sents)\n",
    "#     print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n",
    "\n",
    "# train tagger\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "\n",
    "tagger = train_tagger(brown_tagged_sents)\n",
    "\n",
    "\n",
    "def clean_output(candidates, limit=0):\n",
    "    '''Output clean phrases from freqdist format, and limit characters under count.\n",
    "    '''\n",
    "    keyphrases = []\n",
    "    sum = 0\n",
    "    for (keyphrase, count) in candidates:\n",
    "        tmp = list(keyphrase.split())\n",
    "        kp = \" \".join([ word.replace(\"(\", \"\").replace(\"'\", \"\").replace(\",\", \"\") for word in tmp[0::2] ])\n",
    "        sum += len(kp)\n",
    "        if sum < limit:\n",
    "            keyphrases.append(kp)\n",
    "        else:\n",
    "            return keyphrases\n",
    "    return keyphrases\n",
    "\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "file = \"pride_and_prejudice.txt\"\n",
    "corpus = create_corpus_from_file(file)\n",
    "sents = tokenize_text(corpus)\n",
    "# corpus = create_corpus_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Showing Frequent Terms \n",
    "\n",
    "In section 1, I tried to extract phrases that appear the most in text file with unigram segamentation. The unigrame candidates are pruned by removing punctions, stop words (with some extra modal words), proper noune, and words less than 2 characters, and also by stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "sister\n",
      "family\n",
      "man\n",
      "father\n",
      "day\n",
      "letter\n",
      "mother\n",
      "reply\n",
      "room\n",
      "friend\n",
      "return\n",
      "house\n",
      "manner\n",
      "work\n",
      "love\n",
      "answer\n",
      "pleasure\n",
      "cry\n",
      "subject\n",
      "part\n",
      "aunt\n",
      "daughter\n",
      "dear\n",
      "place\n",
      "walk\n",
      "morning\n",
      "word\n",
      "talk\n",
      "sisters\n",
      "dance\n",
      "ladies\n",
      "brother\n",
      "surprise\n",
      "happiness\n",
      "party\n",
      "opinion\n",
      "attention\n",
      "eye\n",
      "moment\n",
      "character\n",
      "world\n",
      "marriage\n",
      "uncle\n",
      "town\n",
      "smile\n",
      "conversation\n",
      "woman\n",
      "mind\n",
      "kind\n",
      "turn\n",
      "side\n",
      "affection\n",
      "reason\n",
      "life\n",
      "point\n",
      "object\n",
      "behaviour\n",
      "friends\n",
      "meet\n",
      "person\n",
      "husband\n",
      "call\n",
      "honour\n",
      "lady\n",
      "look\n",
      "acquaintance\n",
      "delight\n",
      "pass\n",
      "occasion\n",
      "people\n",
      "cousin\n",
      "doubt\n",
      "idea\n",
      "daughters\n",
      "spirit\n",
      "hop\n",
      "business\n",
      "concern\n",
      "power\n",
      "interest\n",
      "hope\n",
      "pride\n",
      "wife\n",
      "heart\n",
      "manners\n",
      "account\n",
      "visit\n",
      "address\n",
      "promise\n",
      "girls\n",
      "carriage\n",
      "fear\n",
      "endeavour\n",
      "term\n",
      "ladyship\n",
      "question\n",
      "advantage\n",
      "country\n",
      "thing\n",
      "civility\n",
      "respect\n",
      "ball\n",
      "compliment\n",
      "oblige\n",
      "fortune\n",
      "laugh\n",
      "sense\n",
      "gentlemen\n",
      "care\n",
      "officer\n",
      "girl\n",
      "situation\n",
      "leave\n",
      "gentleman\n",
      "years\n",
      "sort\n"
     ]
    }
   ],
   "source": [
    "def build_stop_words():\n",
    "    '''Build stop words from SMART (Salton,1971).  Available at ftp://ftp.cs.cornell.edu/pub/smart/english.stop\n",
    "    '''\n",
    "    stop_words = []\n",
    "    file = \"SmartStoplist.txt\"\n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            tmp = fp.readlines()\n",
    "            stop_words = [ word.replace('\\n', '') for word in tmp ]\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "def extract_unigram(sents):\n",
    "    '''Extract unigram candidates, and prune the candidates.\n",
    "\n",
    "    '''\n",
    "    unigram_raw_candidates = []\n",
    "    unigram_candidates = []\n",
    "    lm = WordNetLemmatizer()\n",
    "    for sent in sents:\n",
    "        unigram_raw_candidates += list(ngrams(sent,1))    \n",
    "    \n",
    "    # Unigram pruning: remove punctions, stop words, words that are capitalized in the first character, words less than 2 characters.\n",
    "    stopwords = build_stop_words()\n",
    "  \n",
    "    unigram_pattern = r\"\\d+|\\'+|\\`+|^[A-Z]\\w*\"\n",
    "    for element in unigram_raw_candidates:\n",
    "        if re.match(unigram_pattern, element[0]) or element[0] in string.punctuation or element[0] in stopwords or len(element[0])<2 :\n",
    "            continue\n",
    "        else:\n",
    "            unigram_candidates.append(lm.lemmatize(element[0], 'v'))\n",
    "    return unigram_candidates\n",
    "\n",
    "\n",
    "raw_candidates = extract_unigram(sents)\n",
    "tagged_candidates = tagger.tag(raw_candidates)\n",
    "candidates = []\n",
    "for (word, pos) in tagged_candidates:\n",
    "    if re.match(r'NN.*', pos):\n",
    "        candidates.append(word)\n",
    "\n",
    "candidates_1 = nltk.FreqDist(candidates).most_common(700)\n",
    "for word in clean_output(candidates_1, 700):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collocations with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. darcy\n",
      "mrs. bennet\n",
      "mr. collins\n",
      "lady catherine\n",
      "mr. bingley\n",
      "mr. bennet\n",
      "miss bingley\n",
      "mr. wickham\n",
      "miss bennet\n",
      "elizabeth wa\n",
      "mrs. gardiner\n",
      "sir william\n",
      "young lady\n",
      "project gutenberg-tm\n",
      "young man\n",
      "miss darcy\n",
      "de bourgh\n",
      "bennet wa\n",
      "mr. gardiner\n",
      "colonel fitzwilliam\n",
      "mrs. collins\n",
      "bingley wa\n",
      "colonel forster\n",
      "miss lucas\n",
      "cried elizabeth\n",
      "electronic work\n",
      "great deal\n",
      "project gutenberg\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "# corpus_words = nltk.word_tokenize(corpus)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# stem sentences\n",
    "lm = WordNetLemmatizer()\n",
    "sents_stemmed = []\n",
    "for sent in sents:\n",
    "    sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "candidates_tmp = []\n",
    "for sent in sents_stemmed:\n",
    "    # tag collocation words with trained brown tagger\n",
    "    finder = BigramCollocationFinder.from_words(tagger.tag(sent))\n",
    "    tmp = finder.nbest(bigram_measures.pmi, 700)\n",
    "    candidates_tmp += tmp\n",
    "\n",
    "candidates_2 = nltk.FreqDist(candidates_tmp).most_common(700)\n",
    "\n",
    "stopwords = build_stop_words()\n",
    "candidates_ = []\n",
    "limit = 700\n",
    "sum = 0\n",
    "for candidate in candidates_2:\n",
    "    # filter out descriptive noun phrases\n",
    "    if re.match(r'NN.*', candidate[0][1][1]):\n",
    "        # filter out punctuation, stopwords\n",
    "        if  (candidate[0][0][0] not in string.punctuation) and (candidate[0][0][0] not in stopwords) and (candidate[0][1][0] not in stopwords):\n",
    "            kp = candidate[0][0][0] + \" \" + candidate[0][1][0]\n",
    "            sum += len(kp)\n",
    "            if sum < limit:\n",
    "                candidates_.append(kp)\n",
    "            else:\n",
    "                break\n",
    "for word in candidates_:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information obtained from Syntax aka Partial Parsing (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young lady\n",
      "young man\n",
      "electronic work\n",
      "great deal\n",
      "whole party\n",
      "sure i\n",
      "literary archive foundation\n",
      "good humour\n",
      "great pleasure\n",
      "good opinion\n",
      "good deal\n",
      "young men\n",
      "young woman\n",
      "fair cousin\n",
      "own family\n",
      "late mr. darcy\n",
      "own feeling\n",
      "whole family\n",
      "own room\n",
      "large party\n",
      "dear lizzy\n",
      "dear sir\n",
      "such thing\n",
      "own child\n",
      "real character\n",
      "short pause\n",
      "full project gutenberg-tm license\n",
      "slight bow\n",
      "such term\n",
      "good spirit\n",
      "humble abode\n",
      "good heaven\n",
      "present i\n",
      "intimate friend\n",
      "own way\n",
      "perfect indifference\n",
      "public domain\n",
      "good news\n",
      "different manner\n",
      "great surprise\n",
      "such circumstance\n",
      "good luck\n",
      "own happiness\n",
      "good girl\n",
      "strong objection\n",
      "sweet girl\n",
      "good joke\n",
      "human nature\n",
      "good fortune\n",
      "whole course\n",
      "own vanity\n",
      "poor lydia\n",
      "low voice\n",
      "such case\n",
      "long time\n",
      "poor mother\n",
      "good sort\n",
      "charming man\n",
      "disagreeable man\n"
     ]
    }
   ],
   "source": [
    "txt_tagged = []\n",
    "\n",
    "# Stem words. Without stemming, top keyphrases consist of phrases with same structure but different forms.\n",
    "lm = WordNetLemmatizer()\n",
    "sents_stemmed = []\n",
    "for sent in sents:\n",
    "    sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "for sent in sents_stemmed:\n",
    "    txt_tagged.append(tagger.tag(sent))\n",
    "\n",
    "# descriptive keyphrases pattern\n",
    "dkp = nltk.RegexpParser('DK: {(<JJ>* <NN.*>+ <IN>)? <JJ>+ <NN.*>+}') \n",
    "\n",
    "descriptive_keyphrases = []\n",
    "for sent in txt_tagged:\n",
    "    tree = dkp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'DK': \n",
    "            dk = \" \".join(str(e) for e in subtree[0:])\n",
    "            descriptive_keyphrases.append(dk)\n",
    "\n",
    "\n",
    "            \n",
    "candidates = nltk.FreqDist(descriptive_keyphrases).most_common(200)\n",
    "for word in clean_output(candidates, 700):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinple unigram approach does not work well, since the goal of extracting descriptive keyphrases maynot be reflected by a single noun word. \n",
    "\n",
    "The top output of Collocations approach contains title nouns, which can't represent key phrases.\n",
    "\n",
    "Chunking phrase seems to work better, but some keyphrases may have same duplicated marning, such as \"young lady\" and \"young woman\" both are keyphrases extracted, but to some extent, have same meaning. \n",
    "\n",
    "Based on three approaches I tried, I think chunking works best among the three, but still need extra work on thesaurus analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
