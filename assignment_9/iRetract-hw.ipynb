{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrases Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "\n",
    "In this section, the corpus is created either from a text file or a url, which is then split into sentences and tokenized. Other functions include build tagger based on brown corpus, and clean outut under certain characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse\n",
    "import string\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def create_corpus_from_file(file):\n",
    "    '''Create corpus from file\n",
    "    ''' \n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            corpus = fp.read()\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        corpus = \"\"\n",
    "    return corpus\n",
    "\n",
    "def create_corpus_from_url(url):\n",
    "    '''Create corpus from url\n",
    "    ''' \n",
    "    try:\n",
    "        res = urllib.request.urlopen(url)\n",
    "        corpus = res.read().decode('utf-8')\n",
    "    except:\n",
    "        corpus = \"\"\n",
    "        print(\"Can't open url: {0}\".format(url))\n",
    "    finally:\n",
    "        res.close()\n",
    "    return corpus   \n",
    "\n",
    "\n",
    "def tokenize_text(corpus):\n",
    "    '''Split text into sentences and tokenize\n",
    "    '''\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus.replace(\"\\ufeff\", \"\")) \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def build_backoff_tagger(train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    ngram_tagger = build_backoff_tagger(already_tagged_sents)\n",
    "#     print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger\n",
    "\n",
    "# train tagger\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance', 'science_fiction'])\n",
    "\n",
    "tagger = train_tagger(brown_tagged_sents)\n",
    "\n",
    "\n",
    "def clean_output(candidates, limit=0):\n",
    "    '''Output clean phrases from freqdist format, and limit characters under count.\n",
    "    '''\n",
    "    keyphrases = []\n",
    "    sum = 0\n",
    "    for (keyphrase, count) in candidates:\n",
    "        tmp = list(keyphrase.split())\n",
    "        kp = \" \".join([ word.replace(\"(\", \"\").replace(\"'\", \"\").replace(\",\", \"\") for word in tmp[0::2] ])\n",
    "        sum += len(kp)\n",
    "        if sum < limit:\n",
    "            keyphrases.append(kp)\n",
    "        else:\n",
    "            return keyphrases\n",
    "    return keyphrases\n",
    "\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "file = \"pride_and_prejudice.txt\"\n",
    "# file = \"mystery_text_expository_2016.txt\"\n",
    "\n",
    "corpus = create_corpus_from_file(file)\n",
    "sents = tokenize_text(corpus)\n",
    "# corpus = create_corpus_from_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Showing Frequent Terms \n",
    "\n",
    "In section 1, I tried to extract phrases that appear the most in text file with unigram segamentation. The unigrame candidates are pruned by removing punctions, stop words (with some extra modal words), proper noune, and words less than 2 characters, and also by stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "sister\n",
      "family\n",
      "man\n",
      "father\n",
      "day\n",
      "letter\n",
      "mother\n",
      "reply\n",
      "room\n",
      "friend\n",
      "return\n",
      "house\n",
      "manner\n",
      "love\n",
      "work\n",
      "answer\n",
      "pleasure\n",
      "cry\n",
      "subject\n",
      "part\n",
      "aunt\n",
      "daughter\n",
      "dear\n",
      "place\n",
      "morning\n",
      "word\n",
      "walk\n",
      "sisters\n",
      "talk\n",
      "ladies\n",
      "brother\n",
      "surprise\n",
      "happiness\n",
      "party\n",
      "opinion\n",
      "attention\n",
      "eye\n",
      "moment\n",
      "character\n",
      "marriage\n",
      "uncle\n",
      "world\n",
      "smile\n",
      "town\n",
      "conversation\n",
      "mind\n",
      "kind\n",
      "woman\n",
      "affection\n",
      "side\n",
      "reason\n",
      "dance\n",
      "point\n",
      "life\n",
      "object\n",
      "turn\n",
      "behaviour\n",
      "friends\n",
      "person\n",
      "lady\n",
      "honour\n",
      "husband\n",
      "call\n",
      "acquaintance\n",
      "look\n",
      "delight\n",
      "occasion\n",
      "people\n",
      "cousin\n",
      "doubt\n",
      "hope\n",
      "idea\n",
      "pass\n",
      "daughters\n",
      "spirit\n",
      "interest\n",
      "power\n",
      "business\n",
      "pride\n",
      "wife\n",
      "heart\n",
      "manners\n",
      "promise\n",
      "fear\n",
      "girls\n",
      "address\n",
      "visit\n",
      "concern\n",
      "account\n",
      "carriage\n",
      "endeavour\n",
      "term\n",
      "ladyship\n",
      "civility\n",
      "thing\n",
      "country\n",
      "advantage\n",
      "question\n",
      "ball\n",
      "respect\n",
      "compliment\n",
      "oblige\n",
      "fortune\n",
      "laugh\n",
      "care\n",
      "gentlemen\n",
      "girl\n",
      "situation\n",
      "sense\n",
      "officer\n",
      "leave\n",
      "gentleman\n",
      "sort\n",
      "form\n",
      "comfort\n",
      "years\n"
     ]
    }
   ],
   "source": [
    "def build_stop_words():\n",
    "    '''Build stop words from SMART (Salton,1971).  Available at ftp://ftp.cs.cornell.edu/pub/smart/english.stop\n",
    "    '''\n",
    "    stop_words = []\n",
    "    file = \"SmartStoplist.txt\"\n",
    "    try:\n",
    "        with open(file, 'r') as fp:\n",
    "            tmp = fp.readlines()\n",
    "            stop_words = [ word.replace('\\n', '') for word in tmp ]\n",
    "    except:\n",
    "        print(\"Can't open specified file: {0}\".format(file))\n",
    "        \n",
    "    return stop_words\n",
    "\n",
    "def extract_unigram(sents):\n",
    "    '''Extract unigram candidates, and prune the candidates.\n",
    "\n",
    "    '''\n",
    "    unigram_raw_candidates = []\n",
    "    unigram_candidates = []\n",
    "    lm = WordNetLemmatizer()\n",
    "    for sent in sents:\n",
    "        unigram_raw_candidates += list(ngrams(sent,1))    \n",
    "    \n",
    "    # Unigram pruning: remove punctions, stop words, words that are capitalized in the first character, words less than 2 characters.\n",
    "    stopwords = build_stop_words()\n",
    "  \n",
    "    unigram_pattern = r\"\\d+|\\'+|\\`+|^[A-Z]\\w*\"\n",
    "    for element in unigram_raw_candidates:\n",
    "        if re.match(unigram_pattern, element[0]) or element[0] in string.punctuation or element[0] in stopwords or len(element[0])<2 :\n",
    "            continue\n",
    "        else:\n",
    "            unigram_candidates.append(lm.lemmatize(element[0], 'v'))\n",
    "    return unigram_candidates\n",
    "\n",
    "\n",
    "raw_candidates = extract_unigram(sents)\n",
    "tagged_candidates = tagger.tag(raw_candidates)\n",
    "candidates = []\n",
    "for (word, pos) in tagged_candidates:\n",
    "    if re.match(r'NN.*', pos):\n",
    "        candidates.append(word)\n",
    "\n",
    "candidates_1 = nltk.FreqDist(candidates).most_common(700)\n",
    "for word in clean_output(candidates_1, 700):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collocations with bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. darcy\n",
      "mrs. bennet\n",
      "mr. collins\n",
      "lady catherine\n",
      "mr. bingley\n",
      "mr. bennet\n",
      "miss bingley\n",
      "mr. wickham\n",
      "miss bennet\n",
      "elizabeth wa\n",
      "mrs. gardiner\n",
      "sir william\n",
      "young lady\n",
      "project gutenberg-tm\n",
      "de bourgh\n",
      "young man\n",
      "miss darcy\n",
      "bennet wa\n",
      "mr. gardiner\n",
      "mrs. collins\n",
      "colonel fitzwilliam\n",
      "colonel forster\n",
      "bingley wa\n",
      "miss lucas\n",
      "electronic work\n",
      "cried elizabeth\n",
      "great deal\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "# corpus_words = nltk.word_tokenize(corpus)\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# stem sentences\n",
    "lm = WordNetLemmatizer()\n",
    "sents_stemmed = []\n",
    "for sent in sents:\n",
    "    sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "candidates_tmp = []\n",
    "for sent in sents_stemmed:\n",
    "    # tag collocation words with trained brown tagger\n",
    "    finder = BigramCollocationFinder.from_words(tagger.tag(sent))\n",
    "    tmp = finder.nbest(bigram_measures.pmi, 700)\n",
    "    candidates_tmp += tmp\n",
    "\n",
    "candidates_2 = nltk.FreqDist(candidates_tmp).most_common(700)\n",
    "\n",
    "stopwords = build_stop_words()\n",
    "candidates_ = []\n",
    "limit = 700\n",
    "sum = 0\n",
    "for candidate in candidates_2:\n",
    "    # filter out descriptive noun phrases\n",
    "    if re.match(r'NN.*', candidate[0][1][1]):\n",
    "        # filter out punctuation, stopwords\n",
    "        if  (candidate[0][0][0] not in string.punctuation) and (candidate[0][0][0] not in stopwords) and (candidate[0][1][0] not in stopwords):\n",
    "            kp = candidate[0][0][0] + \" \" + candidate[0][1][0]\n",
    "            sum += len(kp)\n",
    "            if sum < limit:\n",
    "                candidates_.append(kp)\n",
    "            else:\n",
    "                break\n",
    "for word in candidates_:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information obtained from Syntax aka Partial Parsing (Chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young lady\n",
      "young man\n",
      "electronic work\n",
      "great deal\n",
      "dear lizzy\n",
      "whole party\n",
      "sure i\n",
      "literary archive foundation\n",
      "good humour\n",
      "dear sir\n",
      "good opinion\n",
      "great pleasure\n",
      "dear aunt\n",
      "young men\n",
      "good deal\n",
      "dear jane\n",
      "fair cousin\n",
      "young woman\n",
      "whole family\n",
      "late mr. darcy\n",
      "dear mr. bennet\n",
      "own family\n",
      "own feeling\n",
      "own room\n",
      "real character\n",
      "dear sister\n",
      "own child\n",
      "large party\n",
      "short pause\n",
      "such thing\n",
      "dear lydia\n",
      "full project gutenberg-tm license\n",
      "dear wickham\n",
      "dear madam\n",
      "good news\n",
      "perfect indifference\n",
      "own way\n",
      "public domain\n",
      "dear charlotte\n",
      "humble abode\n",
      "good heaven\n",
      "present i\n",
      "such term\n",
      "slight bow\n",
      "dear father\n",
      "good spirit\n",
      "intimate friend\n",
      "own vanity\n",
      "dear cousin\n",
      "low voice\n",
      "good sense\n",
      "poor mother\n",
      "different manner\n",
      "dear eliza\n",
      "whole course\n",
      "good wish\n",
      "own happiness\n",
      "disagreeable man\n",
      "particular friend\n"
     ]
    }
   ],
   "source": [
    "txt_tagged = []\n",
    "\n",
    "# Stem words. Without stemming, top keyphrases consist of phrases with same structure but different forms.\n",
    "lm = WordNetLemmatizer()\n",
    "sents_stemmed = []\n",
    "for sent in sents:\n",
    "    sents_stemmed.append([lm.lemmatize(word).lower() for word in sent])\n",
    "\n",
    "for sent in sents_stemmed:\n",
    "    txt_tagged.append(tagger.tag(sent))\n",
    "\n",
    "# descriptive keyphrases pattern\n",
    "dkp = nltk.RegexpParser('DK: {(<JJ>* <NN.*>+ <IN>)? <JJ>+ <NN.*>+}') \n",
    "\n",
    "descriptive_keyphrases = []\n",
    "for sent in txt_tagged:\n",
    "    tree = dkp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'DK': \n",
    "            dk = \" \".join(str(e) for e in subtree[0:])\n",
    "            descriptive_keyphrases.append(dk)\n",
    "\n",
    "\n",
    "            \n",
    "candidates = nltk.FreqDist(descriptive_keyphrases).most_common(200)\n",
    "for word in clean_output(candidates, 700):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unigram approach, I can tell something from the output key phrases. The top top words of keyphrases contain words such as \"man\", \"family\", \"love\", \"cry\", so I may guess the content of this collection are about some sad love story between a man and a lady. However, the unigram approach is not enough to reflect more details of the collection.\n",
    "\n",
    "The collocations method does not work well, since he top output contains too many title nouns like \"mr. darcy\", \"mrs. bennet\", \"mr. collins\", which can tell people in the collection, but fail to tell the story behind the people.\n",
    "\n",
    "Chunking phrase seems to work better, but some keyphrases may have same duplicated marning, such as \"young lady\" and \"young woman\" both are keyphrases extracted, but to some extent, have same meaning. \n",
    "\n",
    "Based on three approaches I tried, I think chunking works best among the three, but still need extra work on thesaurus analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
